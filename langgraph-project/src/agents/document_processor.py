"""
æ–‡æ¡£å¤„ç†Agent
è´Ÿè´£è¯»å–æ–‡æ¡£å¹¶å°†å…¶åˆ†å‰²ä¸ºå¤šä¸ªchunk
"""
import logging
import os
from typing import List
import docx
import tiktoken

from graph.workflow_state import WorkflowState, ProcessStatus
from config.settings import settings

logger = logging.getLogger(__name__)


def count_tokens(text: str) -> int:
    """
    è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡
    
    å‚æ•°:
        text: è¦è®¡ç®—çš„æ–‡æœ¬
        
    è¿”å›:
        tokenæ•°é‡
    """
    try:
        encoding = tiktoken.encoding_for_model("gpt-4")
        return len(encoding.encode(text))
    except Exception as e:
        logger.warning(f"ä½¿ç”¨gpt-4ç¼–ç å¤±è´¥ï¼Œä½¿ç”¨cl100k_base: {str(e)}")
        encoding = tiktoken.get_encoding("cl100k_base")
        return len(encoding.encode(text))


def read_word_doc(doc_path: str) -> str:
    """
    è¯»å–Wordæ–‡æ¡£
    
    å‚æ•°:
        doc_path: æ–‡æ¡£è·¯å¾„
        
    è¿”å›:
        æ–‡æ¡£å†…å®¹
    """
    try:
        document = docx.Document(doc_path)
        full_text = []
        for para in document.paragraphs:
            if para.style and para.style.name.startswith('Heading'):
                level = int(para.style.name.split(' ')[1])
                full_text.append("\n" + "#" * level + " " + para.text.strip())
            else:
                full_text.append(para.text.strip())
        return "\n\n".join(full_text)
    except Exception as e:
        logger.error(f"è¯»å–Wordæ–‡æ¡£å¤±è´¥: {str(e)}", exc_info=True)
        raise ValueError(f"è¯»å–Wordæ–‡æ¡£å¤±è´¥: {str(e)}")


def read_text_file(file_path: str) -> str:
    """
    è¯»å–æ–‡æœ¬æ–‡ä»¶ï¼ˆæ”¯æŒ .txt, .md ç­‰ï¼‰
    
    å‚æ•°:
        file_path: æ–‡ä»¶è·¯å¾„
        
    è¿”å›:
        æ–‡ä»¶å†…å®¹
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        logger.error(f"è¯»å–æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {str(e)}", exc_info=True)
        raise ValueError(f"è¯»å–æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {str(e)}")


def read_document(doc_path: str) -> str:
    """
    è¯»å–æ–‡æ¡£ï¼ˆè‡ªåŠ¨è¯†åˆ«æ–‡ä»¶ç±»å‹ï¼‰
    
    å‚æ•°:
        doc_path: æ–‡æ¡£è·¯å¾„
        
    è¿”å›:
        æ–‡æ¡£å†…å®¹
    """
    _, ext = os.path.splitext(doc_path)
    ext = ext.lower()
    
    if ext in ['.docx', '.doc']:
        return read_word_doc(doc_path)
    elif ext in ['.txt', '.md', '.markdown']:
        return read_text_file(doc_path)
    else:
        # å°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–
        logger.warning(f"æœªçŸ¥æ–‡ä»¶ç±»å‹ {ext}ï¼Œå°è¯•ä½œä¸ºæ–‡æœ¬æ–‡ä»¶è¯»å–")
        return read_text_file(doc_path)


def split_text_into_chunks(text: str, max_tokens: int = 2000, overlap_tokens: int = 200) -> List[str]:
    """
    å°†æ–‡æœ¬åˆ†å‰²æˆå¤šä¸ªchunkï¼ŒæŒ‰tokenæ•°é‡åˆ†å‰²
    
    å‚æ•°:
        text: è¦åˆ†å‰²çš„æ–‡æœ¬
        max_tokens: æ¯ä¸ªchunkçš„æœ€å¤§tokenæ•°
        overlap_tokens: chunkä¹‹é—´çš„é‡å tokenæ•°
        
    è¿”å›:
        åˆ†å‰²åçš„chunkåˆ—è¡¨
    """
    encoding = tiktoken.get_encoding("cl100k_base")
    tokens = encoding.encode(text)
    
    chunks = []
    start = 0
    
    while start < len(tokens):
        # è®¡ç®—å½“å‰chunkçš„ç»“æŸä½ç½®
        end = start + max_tokens
        
        # è·å–å½“å‰chunkçš„tokens
        chunk_tokens = tokens[start:end]
        
        # è§£ç å›æ–‡æœ¬
        chunk_text = encoding.decode(chunk_tokens)
        chunks.append(chunk_text)
        
        # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªchunkï¼Œè€ƒè™‘é‡å 
        start = end - overlap_tokens
        
        # å¦‚æœå‰©ä½™tokensä¸è¶³overlapï¼Œç›´æ¥è·³åˆ°æœ«å°¾
        if start + max_tokens >= len(tokens):
            if start < len(tokens):
                remaining_tokens = tokens[start:]
                remaining_text = encoding.decode(remaining_tokens)
                if remaining_text.strip():  # åªæ·»åŠ éç©ºå†…å®¹
                    chunks.append(remaining_text)
            break
    
    logger.info(f"æ–‡æœ¬åˆ†å‰²å®Œæˆ: æ€»tokens={len(tokens)}, åˆ†å‰²ä¸º{len(chunks)}ä¸ªchunks")
    return chunks


def process_document(state: WorkflowState) -> WorkflowState:
    """
    å¤„ç†æ–‡æ¡£ï¼Œæå–æ–‡æœ¬å†…å®¹å¹¶åˆ†å‰²ä¸ºchunks
    
    å‚æ•°:
        state: å½“å‰å·¥ä½œæµçŠ¶æ€
        
    è¿”å›:
        æ›´æ–°åçš„å·¥ä½œæµçŠ¶æ€
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡æ¡£è·¯å¾„
    if not state.input_doc_path:
        # å¦‚æœæ²¡æœ‰æ–‡æ¡£è·¯å¾„ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ‰©å±•å†…å®¹éœ€è¦åˆ†å—
        if state.expanded_content:
            logger.info("âœ… ä½¿ç”¨æ‰©å±•å†…å®¹è¿›è¡Œåˆ†å—")
            text_content = state.expanded_content
        else:
            logger.warning("âš ï¸ æ—¢æ²¡æœ‰æä¾›æ–‡æ¡£è·¯å¾„ï¼Œä¹Ÿæ²¡æœ‰æ‰©å±•å†…å®¹")
            # ä¸è®¾ç½®ä¸ºå¤±è´¥ï¼Œè®©å·¥ä½œæµç»§ç»­
            return state
    else:
        # æœ‰æ–‡æ¡£è·¯å¾„ï¼Œè¯»å–æ–‡æ¡£
        if not os.path.exists(state.input_doc_path):
            state.error_message = f"æ–‡æ¡£è·¯å¾„ä¸å­˜åœ¨: {state.input_doc_path}"
            state.status = ProcessStatus.FAILED
            return state
        
        try:
            logger.info(f"ğŸ“– å¼€å§‹è¯»å–æ–‡æ¡£: {state.input_doc_path}")
            text_content = read_document(state.input_doc_path)
            
            # å¦‚æœå·²æœ‰æ‰©å±•å†…å®¹ï¼Œåˆå¹¶
            if state.expanded_content:
                logger.info("ğŸ“ åˆå¹¶æ‰©å±•å†…å®¹å’Œæ–‡æ¡£å†…å®¹")
                text_content = state.expanded_content + "\n\n" + text_content
            else:
                state.expanded_content = text_content
                
            logger.info(f"âœ… æ–‡æ¡£è¯»å–æˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(text_content)} å­—ç¬¦")
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
            state.error_message = f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}"
            state.status = ProcessStatus.FAILED
            return state
    
    try:
        # åˆ†å‰²æ–‡æœ¬ä¸ºchunks
        logger.info(f"ğŸ“„ å¼€å§‹åˆ†å‰²æ–‡æœ¬ï¼Œæœ€å¤§tokenæ•°: {state.max_chunk_tokens}")
        chunks = split_text_into_chunks(
            text_content, 
            max_tokens=state.max_chunk_tokens,
            overlap_tokens=200  # å¯ä»¥é…ç½®
        )
        
        # è®¡ç®—æ¯ä¸ªchunkçš„tokenæ•°
        chunk_tokens = [count_tokens(chunk) for chunk in chunks]
        
        # ä¿å­˜åˆ°çŠ¶æ€
        state.text_chunks = chunks
        state.chunk_token_counts = chunk_tokens
        
        # æ‰“å°åˆ†å—ä¿¡æ¯
        print("\n" + "="*80)
        print(f"ğŸ“„ æ–‡æ¡£åˆ†å—å®Œæˆ")
        print("="*80)
        print(f"æ€»å­—ç¬¦æ•°: {len(text_content)}")
        print(f"æ€»tokenæ•°: {sum(chunk_tokens)}")
        print(f"åˆ†å—æ•°é‡: {len(chunks)}")
        print(f"å¹³å‡æ¯å—tokenæ•°: {sum(chunk_tokens) // len(chunks) if chunks else 0}")
        print("\nå„åˆ†å—tokenæ•°:")
        for i, tokens in enumerate(chunk_tokens, 1):
            print(f"  Chunk {i}: {tokens} tokens")
        print("="*80 + "\n")
        
        logger.info(f"âœ… æ–‡æ¡£åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªchunks")
        
        return state
        
    except Exception as e:
        logger.error(f"âŒ æ–‡æœ¬åˆ†å—å¤±è´¥: {str(e)}", exc_info=True)
        state.error_message = f"æ–‡æœ¬åˆ†å—å¤±è´¥: {str(e)}"
        state.status = ProcessStatus.FAILED
        return state