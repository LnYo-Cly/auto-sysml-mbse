"""
èåˆAgent - è´Ÿè´£æ•´åˆæ‰€æœ‰SysMLå›¾çš„JSONè¾“å‡ºï¼Œæ„å»ºç»Ÿä¸€çš„çŸ¥è¯†å›¾è°±
åŸºäº master-2/step5_relationship_building/run_step5_final_pipeline.py æ”¹é€ 
"""
import logging
import json
import os
from typing import Dict, Any, List
from datetime import datetime
import glob
from graph.workflow_state import WorkflowState, ProcessStatus
from config.settings import settings

logger = logging.getLogger(__name__)

def collect_diagram_json_paths(state: WorkflowState) -> List[str]:
    """
    æ”¶é›†æ‰€æœ‰å·²å®Œæˆä»»åŠ¡çš„ JSON æ–‡ä»¶è·¯å¾„
    """
    json_paths = []
    
    # ç­–ç•¥ 1: å°è¯•ä»ä»»åŠ¡ç»“æœä¸­è·å–è·¯å¾„ (æ ‡å‡†æµç¨‹)
    for task in state.assigned_tasks:
        if task.status == ProcessStatus.COMPLETED and task.result:
            if isinstance(task.result, dict):
                if "saved_file" in task.result:
                    json_paths.append(task.result["saved_file"])
                elif "json_path" in task.result:
                    json_paths.append(task.result["json_path"])
            elif isinstance(task.result, str) and task.result.endswith(".json"):
                json_paths.append(task.result)
    
    # ç­–ç•¥ 2: å…œåº•æœºåˆ¶ - å¦‚æœä»»åŠ¡ç»“æœä¸­æ²¡æœ‰è·¯å¾„ï¼Œæ‰«æé»˜è®¤è¾“å‡ºç›®å½•
    if not json_paths:
        logger.warning("âš ï¸ ä»ä»»åŠ¡ç»“æœä¸­æœªæå–åˆ°JSONè·¯å¾„ï¼Œå¯åŠ¨å…œåº•ç­–ç•¥ï¼šæ‰«æé»˜è®¤è¾“å‡ºç›®å½•...")
        
        try:
            # è·å–é¡¹ç›®æ ¹ç›®å½• (å‡è®¾ç»“æ„ä¸º src/agents/fusion_agent.py)
            current_dir = os.path.dirname(os.path.abspath(__file__))
            src_dir = os.path.dirname(current_dir)
            project_root = os.path.dirname(src_dir)
            base_output_dir = os.path.join(project_root, "data", "output")
            
            diagram_dirs = [
                "activity_diagrams", "block_diagrams", "requirement_diagrams",
                "state_machine_diagrams", "usecase_diagrams", "parametric_diagrams",
                "sequence_diagrams"
            ]
            
            for d_dir in diagram_dirs:
                pattern = os.path.join(base_output_dir, d_dir, "*.json")
                found_files = glob.glob(pattern)
                if found_files:
                    json_paths.extend(found_files)
                    logger.info(f"   - åœ¨ {d_dir} ä¸­æ‰«æåˆ° {len(found_files)} ä¸ªæ–‡ä»¶")
                    
        except Exception as e:
            logger.error(f"âŒ æ‰«æç›®å½•å¤±è´¥: {e}")

    # å»é‡å¹¶è¿‡æ»¤ä¸å­˜åœ¨çš„æ–‡ä»¶
    valid_paths = []
    seen = set()
    for p in json_paths:
        if p and os.path.exists(p) and p not in seen:
            valid_paths.append(p)
            seen.add(p)
    
    logger.info(f"ğŸ“Š æœ€ç»ˆæ”¶é›†åˆ° {len(valid_paths)} ä¸ªæœ‰æ•ˆçš„JSONæ–‡ä»¶")
    return valid_paths


def run_fusion_pipeline(json_paths: List[str]) -> Dict[str, Any]:
    """
    æ‰§è¡Œå®Œæ•´çš„èåˆæµç¨‹
    
    è¿™æ˜¯ master-2/step5_relationship_building/run_step5_final_pipeline.py çš„ main() å‡½æ•°æ”¹é€ ç‰ˆæœ¬
    
    å‚æ•°:
        json_paths: JSONæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        
    è¿”å›:
        èåˆç»“æœå­—å…¸
    """
    print("--- æœ€ç»ˆèåˆç®¡é“: æ­¥éª¤ 1-7 (åŒ…å«å…³ç³»é‡å»º + æ¨¡å‹ç»Ÿä¸€) ---")
    
    # å¯¼å…¥ master-2 çš„æ¨¡å—ï¼ˆæ‚¨éœ€è¦å…ˆè¿ç§»è¿™äº›æ¨¡å—åˆ°é¡¹ç›®ä¸­ï¼‰
    try:
        from fusion.jsontokey import CanonicalKeyGenerator, load_json_files
        from fusion.neo4j_fusion_manager import Neo4jFusionManager
        from fusion.semantic_fusion_manager import SemanticFusionManager
        from connections.database_connectors import close_connections
        from exports.neo4j_to_json import JsonReverser 
    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥èåˆæ¨¡å—å¤±è´¥: {e}")
        return {"status": "error", "message": f"æ¨¡å—å¯¼å…¥å¤±è´¥: {e}"}
    
    # --- æ­¥éª¤ 1: å‡†å¤‡æ•°æ® ---
    print("\n[1/7] æ­£åœ¨åŠ è½½ã€è§£æå¹¶ç”Ÿæˆè§„èŒƒé”®...")
    try:
        # ä½¿ç”¨æ”¶é›†åˆ°çš„JSONæ–‡ä»¶è·¯å¾„ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç çš„è·¯å¾„
        master_element_list = load_json_files(*json_paths)
        all_elements_map = {elem['id']: elem for elem in master_element_list}
        key_generator = CanonicalKeyGenerator(master_element_list)
        elements_with_keys = key_generator.generate_all_keys()
        print(f"  âœ… æ•°æ®å‡†å¤‡å®Œæˆã€‚å…±å¤„ç† {len(master_element_list)} ä¸ªå…ƒç´ ã€‚")
        logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(master_element_list)} ä¸ªå…ƒç´ ")
    except FileNotFoundError as e:
        error_msg = f"æ–‡ä»¶æœªæ‰¾åˆ°: {e}"
        print(f"âŒ é”™è¯¯: {error_msg}")
        logger.error(f"âŒ {error_msg}")
        return {"status": "error", "message": error_msg}
    except Exception as e:
        error_msg = f"æ•°æ®å‡†å¤‡å¤±è´¥: {e}"
        print(f"âŒ é”™è¯¯: {error_msg}")
        logger.error(f"âŒ {error_msg}", exc_info=True)
        return {"status": "error", "message": error_msg}

    neo4j_manager = None
    semantic_manager = None
    canonical_key_remap = {}  # åˆå§‹åŒ–é‡æ˜ å°„è¡¨
    
    try:
        # --- æ­¥éª¤ 2: åˆå§‹åŒ–ç®¡ç†å™¨ ---
        print("\n[2/7] æ­£åœ¨åˆå§‹åŒ–æ‰€æœ‰ç®¡ç†å™¨...")
        neo4j_manager = Neo4jFusionManager()
        semantic_manager = SemanticFusionManager()
        print("  âœ… ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆã€‚")
        logger.info("âœ… ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

        # --- æ­¥éª¤ 3: æ¸…ç©ºæ•°æ®åº“å¹¶è®¾ç½®çº¦æŸ ---
        print("\n[3/7] æ­£åœ¨æ¸…ç©ºæ•°æ®åº“å¹¶è®¾ç½®çº¦æŸ (ä¸ºäº†å¹‚ç­‰æ€§)...")
        neo4j_manager._execute_write("MATCH (n) DETACH DELETE n")
        print("  - æ—§æ•°æ®å·²æ¸…ç©ºã€‚")
        logger.info("- æ—§æ•°æ®å·²æ¸…ç©º")
        neo4j_manager.setup_constraints(master_element_list)
        print("  âœ… çº¦æŸè®¾ç½®å®Œæˆã€‚")
        logger.info("âœ… çº¦æŸè®¾ç½®å®Œæˆ")

        # --- æ­¥éª¤ 4: æ‰¹é‡å¹¶è¡Œè¿­ä»£èåˆ ---
        print("\n[4/7] å¼€å§‹æ‰¹é‡å¹¶è¡Œè¿­ä»£èåˆï¼ˆå‘é‡å¹¶è¡Œç”Ÿæˆ + æ‰¹é‡ä»²è£ + æ‰¹é‡å†™å…¥ï¼‰...")
        logger.info("ğŸ”„ å¼€å§‹æ‰¹é‡å¹¶è¡Œè¿­ä»£èåˆ...")
        
        processed_count = 0
        similar_count = 0
        
        # å°†å­—å…¸è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿åˆ†å—
        all_items = list(elements_with_keys.items())
        batch_size = settings.batch_size  # æ¯æ‰¹å¤„ç† 20 ä¸ªå…ƒç´ 
        total_batches = (len(all_items) + batch_size - 1) // batch_size
        
        logger.info(f"ğŸ“Š æ€»å…± {len(all_items)} ä¸ªå…ƒç´ ï¼Œåˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡å¤„ç†")
        
        for batch_idx in range(0, len(all_items), batch_size):
            batch_items = all_items[batch_idx : batch_idx + batch_size]
            current_batch_num = batch_idx // batch_size + 1
            
            print(f"\n  --- æ‰¹æ¬¡ {current_batch_num}/{total_batches} (å¤§å°: {len(batch_items)}) ---")
            logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {current_batch_num}/{total_batches}")
            
            # 1. å‡†å¤‡æ•°æ®å’Œæ–‡æœ¬
            batch_data = []
            texts_to_embed = []
            
            for original_id, canonical_key in batch_items:
                element = all_elements_map.get(original_id)
                if not element:
                    continue
                
                # æ„å»º Embedding æ–‡æœ¬ (é€»è¾‘åŒåŸ store_element_embedding)
                name = element.get('name', canonical_key.split('::')[-1])
                desc = element.get('description', '')
                if isinstance(desc, dict):
                    desc = json.dumps(desc, ensure_ascii=False)
                
                type_ = element.get('type', 'Unknown')
                text = f"A {type_} named {name}: {desc}" if desc else f"A {type_} named {name}"
                
                batch_data.append({
                    'element': element,
                    'key': canonical_key,
                    'text': text,
                    'type': type_,
                    'name': name
                })
                texts_to_embed.append((text, name))
            
            if not batch_data:
                continue
            
            # 2. å¹¶è¡Œç”Ÿæˆå‘é‡
            print(f"    ğŸš€ å¹¶è¡Œç”Ÿæˆ {len(texts_to_embed)} ä¸ªå‘é‡...")
            embeddings = semantic_manager.get_embeddings_parallel(texts_to_embed)
            
            # 3. å‘é‡æœç´¢ & æ”¶é›†ä»²è£å€™é€‰
            arbitration_queue = []  # å­˜æ”¾ (index_in_batch, item, candidate_info)
            
            for idx, embedding in enumerate(embeddings):
                if not embedding:
                    # å‘é‡ç”Ÿæˆå¤±è´¥ï¼Œæ ‡è®°ä¸ºæ–°å…ƒç´ 
                    batch_data[idx]['is_new'] = True
                    continue
                
                item = batch_data[idx]
                # è°ƒç”¨æ–°æ–¹æ³•ï¼ŒåªæŸ¥ä¸å­˜
                candidate = semantic_manager.search_candidate_only(
                    embedding, 
                    item['type'], 
                    item['key']
                )
                
                if candidate:
                    # åŠ å…¥ä»²è£é˜Ÿåˆ—
                    arbitration_queue.append((idx, item, candidate))
                else:
                    # æ— ç›¸ä¼¼é¡¹ï¼Œç›´æ¥æ ‡è®°ä¸ºæ–°å…ƒç´ 
                    item['is_new'] = True
            
            print(f"    ğŸ” æ‰¾åˆ° {len(arbitration_queue)} ä¸ªç›¸ä¼¼å€™é€‰ï¼Œå‡†å¤‡æ‰¹é‡ä»²è£...")
            
            # 4. æ‰¹é‡ LLM ä»²è£
            if arbitration_queue:
                pairs_to_judge = []
                for _, item, cand in arbitration_queue:
                    pairs_to_judge.append((
                        item['key'], 
                        item['element'].get('description', ''),
                        cand['key'], 
                        cand['description']
                    ))
                
                # ä¸€æ¬¡æ€§è£æ–­
                print(f"    ğŸ¤– æ‰¹é‡ä»²è£ {len(pairs_to_judge)} å¯¹å®ä½“...")
                results = semantic_manager.llm_arbiter.batch_are_they_the_same_entity(pairs_to_judge)
                
                # åº”ç”¨ç»“æœ
                for res_idx, is_same in enumerate(results):
                    q_idx, item, cand = arbitration_queue[res_idx]
                    if is_same:
                        # åˆ¤å®šä¸ºç›¸åŒï¼Œè¿›è¡Œèåˆæ˜ å°„
                        canonical_key_remap[item['key']] = cand['key']
                        item['is_new'] = False
                        similar_count += 1
                        logger.info(f"  ğŸ”— èåˆ: {item['key']} -> {cand['key']}")
                    else:
                        item['is_new'] = True
            
            # 5. æ‰¹é‡å†™å…¥ (Neo4j & VectorDB)
            new_elements_in_batch = [item for item in batch_data if item.get('is_new', True)]
            print(f"    ğŸ’¾ æ‰¹é‡å†™å…¥ {len(new_elements_in_batch)} ä¸ªæ–°å…ƒç´ ...")
            
            for idx, item in enumerate(batch_data):
                if item.get('is_new', True):
                    # å†™å…¥ Neo4j
                    neo4j_manager.fuse_element(item['element'], item['key'])
                    
                    # å†™å…¥å‘é‡æ•°æ®åº“
                    if embeddings[idx]:
                        semantic_manager.store_embedding_direct(
                            item['key'], 
                            item['element'], 
                            embeddings[idx]
                        )
                    
                    processed_count += 1
            
            print(f"    âœ… æ‰¹æ¬¡ {current_batch_num} å®Œæˆ: æ–°å¢ {len(new_elements_in_batch)} ä¸ªå…ƒç´ ")
        
        print(f"\n  âœ… æ‰¹é‡è¿­ä»£èåˆå®Œæˆã€‚å¤„ç†äº† {processed_count} ä¸ªæ–°å…ƒç´ ï¼Œè·³è¿‡ {similar_count} ä¸ªç›¸ä¼¼å…ƒç´ ã€‚")
        logger.info(f"âœ… æ‰¹é‡è¿­ä»£èåˆå®Œæˆ: æ–°å…ƒç´ ={processed_count}, ç›¸ä¼¼å…ƒç´ ={similar_count}")

        # --- æ­¥éª¤ 5: å…³ç³»é‡å»º ---
        print("\n[5/7] å¼€å§‹å…³ç³»é‡å»ºæµç¨‹...")
        logger.info("ğŸ”— å¼€å§‹å…³ç³»é‡å»º...")
        neo4j_manager.rebuild_relationships(
            all_elements_map,
            elements_with_keys,
            canonical_key_remap
        )
        print("  âœ… å…³ç³»é‡å»ºå®Œæˆã€‚")
        logger.info("âœ… å…³ç³»é‡å»ºå®Œæˆ")
        
        # --- æ­¥éª¤ 6: æ¨¡å‹ç»Ÿä¸€ ---
        print("\n[6/7] æ­£åœ¨ç»Ÿä¸€æ¨¡å‹æ ¹...")
        logger.info("ğŸ¯ æ­£åœ¨ç»Ÿä¸€æ¨¡å‹æ ¹...")
        neo4j_manager.unify_models(
            master_model_original_id="master-model",
            master_model_name="Model"  # æ‚¨å¯ä»¥è‡ªå®šä¹‰åç§°
        )
        print("  âœ… æ¨¡å‹ç»Ÿä¸€å®Œæˆã€‚")
        logger.info("âœ… æ¨¡å‹ç»Ÿä¸€å®Œæˆ")
        
        # --- æ­¥éª¤ 7: å¯¼å‡ºæœ€ç»ˆç»“æœï¼ˆå¯é€‰ï¼‰ ---
        print("\n[7/7] æ­£åœ¨å¯¼å‡ºèåˆç»“æœ...")
        logger.info("ğŸ’¾ æ­£åœ¨å¯¼å‡ºèåˆç»“æœ...")
        
        # ä»Neo4jå¯¼å‡ºç»Ÿä¸€çš„JSONï¼ˆæ‚¨å¯èƒ½éœ€è¦ä» master-2 å®ç°è¿™ä¸ªåŠŸèƒ½ï¼‰
        try:
            # è¿™é‡Œå‡è®¾æœ‰ä¸€ä¸ªå¯¼å‡ºå‡½æ•°ï¼Œæ‚¨éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
            final_json = neo4j_manager.export_to_json()
            logger.info("âœ… èåˆç»“æœå¯¼å‡ºå®Œæˆ")
        except AttributeError:
            # å¦‚æœæ²¡æœ‰å¯¼å‡ºåŠŸèƒ½ï¼Œè¿”å›ç»Ÿè®¡ä¿¡æ¯
            logger.warning("âš ï¸ æœªå®ç°å¯¼å‡ºåŠŸèƒ½ï¼Œè¿”å›ç»Ÿè®¡ä¿¡æ¯")
            final_json = {
                "total_elements": len(master_element_list),
                "processed_elements": processed_count,
                "similar_elements": similar_count,
                "canonical_key_remap": canonical_key_remap
            }
        
        # --- æ­¥éª¤ 7: å¯¼å‡ºæœ€ç»ˆç»“æœ ---
        print("\n[7/7] æ­£åœ¨ä»Neo4jå¯¼å‡ºèåˆåçš„JSON...")
        logger.info("ğŸ’¾ æ­£åœ¨ä»Neo4jå¯¼å‡ºèåˆåçš„JSON...")
        
        # âœ… ä½¿ç”¨ JsonReverser ä» Neo4j å¯¼å‡ºå®Œæ•´çš„ JSON
        try:
            reverser = JsonReverser()
            final_json = reverser.reconstruct_json()
            logger.info("âœ… èåˆç»“æœå¯¼å‡ºæˆåŠŸ")
            print("  âœ… JSONå¯¼å‡ºæˆåŠŸ")
        except Exception as export_error:
            logger.warning(f"âš ï¸ JSONå¯¼å‡ºå¤±è´¥ï¼Œä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯æ›¿ä»£: {export_error}")
            print(f"  âš ï¸ JSONå¯¼å‡ºå¤±è´¥: {export_error}")
            final_json = {
                "model": [],
                "elements": [],
                "statistics": {
                    "total_elements": len(master_element_list),
                    "processed_elements": processed_count,
                    "similar_elements": similar_count
                }
            }
        
        print("\nâœ… [7/7] ç®¡é“æ‰§è¡ŒæˆåŠŸï¼")
        logger.info("âœ… èåˆç®¡é“æ‰§è¡ŒæˆåŠŸ")
        
        return {
            "status": "success",
            "result": final_json,
            "statistics": {
                "total_elements": len(master_element_list),
                "processed_elements": processed_count,
                "similar_elements": similar_count,
                "total_fused_elements": len(final_json.get("elements", [])) if isinstance(final_json, dict) else 0
            }
        }
        
    except (ConnectionError, Exception) as e:
        error_msg = f"ç®¡é“æ‰§è¡Œå¤±è´¥: {e}"
        print(f"\nâŒ {error_msg}")
        logger.error(f"âŒ {error_msg}", exc_info=True)
        return {"status": "error", "message": error_msg}
    
    finally:
        print("\næ­£åœ¨å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥...")
        logger.info("æ­£åœ¨å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥...")
        close_connections()
        print("âœ… æ¸…ç†å®Œæˆã€‚")
        logger.info("âœ… æ¸…ç†å®Œæˆ")


def fusion_agent(state: WorkflowState) -> WorkflowState:
    """
    èåˆAgentä¸»å‡½æ•° - LangGraphå·¥ä½œæµèŠ‚ç‚¹
    
    åŠŸèƒ½:
        1. æ”¶é›†æ‰€æœ‰å·²å®Œæˆä»»åŠ¡çš„JSONè¾“å‡º
        2. æ‰§è¡Œèåˆæµç¨‹ï¼ˆNeo4j + è¯­ä¹‰èåˆï¼‰
        3. ä¿å­˜èåˆç»“æœ
        
    å‚æ•°:
        state: å½“å‰å·¥ä½œæµçŠ¶æ€
        
    è¿”å›:
        æ›´æ–°åçš„å·¥ä½œæµçŠ¶æ€
    """
    logger.info("=" * 80)
    logger.info("ğŸ”— èåˆAgentå¼€å§‹å·¥ä½œ")
    logger.info("=" * 80)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·²å®Œæˆçš„ä»»åŠ¡
    completed_tasks = [
        t for t in state.assigned_tasks 
        if t.status == ProcessStatus.COMPLETED
    ]
    
    if not completed_tasks:
        logger.warning("âš ï¸ æ²¡æœ‰å·²å®Œæˆçš„ä»»åŠ¡ï¼Œè·³è¿‡èåˆæ­¥éª¤")
        state.fusion_status = "skipped"
        state.fusion_message = "æ²¡æœ‰å·²å®Œæˆçš„ä»»åŠ¡"
        return state
    
    try:
        # 1. æ”¶é›†æ‰€æœ‰å›¾çš„JSONæ–‡ä»¶è·¯å¾„
        json_paths = collect_diagram_json_paths(state)
        
        if not json_paths:
            logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„JSONæ–‡ä»¶ï¼Œè·³è¿‡èåˆæ­¥éª¤")
            state.fusion_status = "skipped"
            state.fusion_message = "æ²¡æœ‰å¯ç”¨çš„JSONæ–‡ä»¶"
            return state
        
        # 2. æ‰§è¡Œèåˆæµç¨‹
        fusion_result = run_fusion_pipeline(json_paths)
        
        if fusion_result["status"] == "success":
            # 3. ä¿å­˜èåˆç»“æœ
            output_dir = os.path.join(
                os.path.dirname(os.path.dirname(__file__)), 
                "data", "output", "fusion"
            )
            os.makedirs(output_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = os.path.join(output_dir, f"fused_model_{timestamp}.json")
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(fusion_result["result"], f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… èåˆç»“æœå·²ä¿å­˜: {output_path}")
            
            # æ›´æ–°çŠ¶æ€
            state.fusion_status = "completed"
            state.fusion_output_path = output_path
            state.fusion_statistics = fusion_result.get("statistics", {})
            
        else:
            logger.error(f"âŒ èåˆå¤±è´¥: {fusion_result.get('message')}")
            state.fusion_status = "failed"
            state.fusion_message = fusion_result.get('message')
    
    except Exception as e:
        logger.error(f"âŒ èåˆAgentå¼‚å¸¸: {e}", exc_info=True)
        state.fusion_status = "failed"
        state.fusion_message = str(e)
    
    logger.info("=" * 80)
    logger.info(f"ğŸ”— èåˆAgentå®Œæˆï¼ŒçŠ¶æ€: {state.fusion_status}")
    logger.info("=" * 80)
    
    return state